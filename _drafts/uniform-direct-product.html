---
layout: post
title: "Constructive Hardness Amplification via Uniform Direct Product"
authorid: preetum
---

<script type="text/javascript">
    // javascript for toggling sidenotes
    function toggle_display_nojump(id) {
       event.preventDefault();
       var e = document.getElementById(id);
       if(e.style.display == 'block')
          e.style.display = 'none';
       else
          e.style.display = 'block';

      return false; // prevent default action of jumping to anchor
    }
</script>
<div style='display:none;'><script type='math/tex'> \renewcommand\qedsymbol{$\blacksquare$}
\newcommand{\1}{\mathbb{1}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\x}{\times}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathop{\mathbb{E}}}
\renewcommand{\bar}{\overline}
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\eps}{\epsilon}
\newcommand{\bmqty}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\innp}[1]{\langle #1 \rangle}
\DeclareMathOperator{\rank}{rank}
\newcommand{\note}[1]{&&\text{(#1)}} % use this to annotate eqns in align environments.
\DeclareMathOperator*{\poly}{poly}
\newcommand{\TODO}[1][TODO]{\textcolor{red}{#1}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\ox}{\otimes}
\newcommand{\bad}{\text{BAD}}
\newcommand{\good}{\text{GOOD}}
 </script></div>
<p>
<em>This post was motivated by trying to understand the recent paper &ldquo;Learning Algorithms from Natural Proofs&rdquo;, by Carmosino-Impagliazzo-Kabanets-Kolokolova [<a href='#ref-CIKK16'>CIKK16</a>]. They crucially use the fact that several results in hardness amplification can be made constructive. In this post, we will look at the Uniform Direct Product Theorem of Impagliazzo-Jaiswal-Kabanets-Wigderson [<a href='#ref-IJKW10'>IJKW10</a>]. We will state the original theorem and algorithm of [<a href='#ref-IJKW10'>IJKW10</a>], then we will present a simpler analysis for a (weaker) non-uniform version of their algorithm, which contains some of the main ideas. </em>
<p>
For a given function $f: \{0, 1\}^n \to \{0, 1\}^\ell$, say a circuit $C$ &ldquo;$\eps$-computes $f$&rdquo; if $C$ computes $f$ correctly on at least $\eps$-fraction of inputs. That is, $\Pr_x[C(x) = f(x)] \geq \epsilon$. We are interested in the following kind of direct product theorem (informally): &ldquo;If function $f$ cannot be $\eps$-computed by any small circuit $C$, then the direct-product $f^{\ox k}(x_1, x_2, \dots x_k) := (f(x_1), f(x_2), \dots, f(x_k))$ cannot be computed better than roughly $\eps^k$ by any similarly small circuit.&rdquo; <sup class='footnotemark'><a href='#footnote1' onclick="toggle_display_nojump('footnote1');">1</a></sup><span class='sidenote' id='footnote1'><a name='footnote1' href='#footnote1'>1.</a>  If this seems trivial, consider the $k=2$ case. We want to show that if $\Pr_x[C(x) = f(x)] \leq \epsilon$ for all small circuits $C$, then $\Pr_{x, y}[C'(x, y) = (f(x), f(y))] \lesssim \eps^2$ for all similarly small circuits $C'$. This is clearly true if the circuit $C'$ operates independently on its inputs, but not as clear otherwise (eg, the correctness of $C'$-s two outputs could be highly correlated). Indeed, proofs of the direct-product theorem take advantage of this correlation.  </span>
<p>
This is usually proved<sup class='footnotemark'><a href='#footnote2' onclick="toggle_display_nojump('footnote2');">2</a></sup><span class='sidenote' id='footnote2'><a name='footnote2' href='#footnote2'>2.</a> See the last section for good references to prior proofs. </span> in contrapositive, by showing: If there exists a circuit $C'$ that $\eps^k$-computes $f^{\ox k}$, then there exists a similarly-sized circuit $C$ that $\eps$-computes $f$. The very interesting part is, this amplification can be made fully constructive, by a simple algorithm.
<!--more-->
<blockquote><b>Theorem 1 ([<a href='#ref-IJKW10'>IJKW10</a>], and Theorem 4.1 [<a href='#ref-CIKK16'>CIKK16</a>])</b> <em> <a name="thmuniformDP"></a> Let $k \in \N, \eps &gt; 0$. There is a (uniform) PPT algorithm $\A$ with the following guarantees:

<ul> <li> <b>Input:</b> A circuit $C'$ that $\eps$-computes $f^{\ox k}$ for some function $f:\{0,1\}^n \to \{0, 1\}^\ell$. <li> <b>Output:</b> With probability $\Omega(\eps)$, output a circuit $C$ that $(1-\delta)$-computes $f$.
</ul>

 for $\delta = O(\log(1/\eps)/k)$. In particular, $(1-\delta) = \eps^{O(1/k)}$. The circuit $C$ is of size $|C'|\poly(n, k, \log(1/\delta), 1/\eps)$. </em></blockquote>

<p>


<p>
Note that we can only hope to construct the good circuit with probability $\Omega(\eps)$, since unique decoding is impossible: the circuit $C'$ may $\eps$-compute up to $(1/\eps)$ different functions $f$ (agreeing with a different function on each $\eps$-fraction of its inputs).
<p>
<h2 class='tex'>1. Uniform Version </h2>
<p>
The algorithm for Theorem <a href="#thmuniformDP">1</a> is: <div class='framed'> $\mathcal{A}(C')$:
<p>
Input: A circuit $C'$ that $\eps$-computes the direct-product $f^{\ox k}$.

<ol> <li> Pick $k$ iid random inputs $x_i \in \{0, 1\}^n$, let $\vec b = (x_1, \dots, x_k)$, and evaluate $C'(\vec b)$. <li> Pick a random subset $A \subset \{x_1, \dots, x_k\}$ of size $k/2$. Record $v := C'(\vec b)|_A$ as the answers of $C'$ on the inputs in $A$. <li> Output the circuit $C_{A, v}$ defined below (with the values $v$ on the subset $A$ hardcoded).
</ol>

 </div>
<p>
$C_{A, v}$ is defined as the randomized circuit:<br/>
 <div class='framed'> $C_{A, v}(x)$:
<p>
On input $x \in \{0, 1\}^n$, check if $x \in A$, in which case output $v|_x$ (the hardcoded value of $x$ according to $v$). Otherwise, repeat the following $T = O(\log(1/\delta)/\epsilon)$ times.

<ol> <li> Sample $(k/2 - 1)$ additional iid random strings $\{y_j\}$, each $y_j \in \{0, 1\}^n$, and let $\vec b := (x, A, \{y_j\})$ be the tuple of $k$ strings. <li> Evaluate $C'(\pi(\vec b))$ for a random permutation $\pi$ of the $k$ inputs. <li> If the answers of $C'$ restricted to $A$ agree with the hardcoded values $v$, then output $C'(\pi(\vec b))|_x$, (the answer $C'$ gave for $x$), and stop.
</ol>

 Output an error if no output is produced after $T$ iterations. </div>
<p>
<b>Intuition:</b> Suppose the values $v$ returned when the Algorithm queries $C'(b)$ are actually correct. That is, $v|_x = f(x)$ for all $x \in A$. Then, the circuit $C_{A, v}$ evaluates $C'$ on input $\vec b = (b_1, \dots b_k)$, and it knows the correct value of $f(b_i)$ is on half of these coordinates. So, $C_{A, v}(x)$ tries to estimate whether a random point $C'(\vec b)$ is correct or not, based on if it agrees on the known subset of coordinates. The idea is that a value of $C'(\vec b)$ that is wrong on many coordinates is unlikely to pass this test. (See [<a href='#ref-IJKW10'>IJKW10</a>] for the full proof).
<p>
Now, in the remainder of this note, we will develop and prove a simpler (weaker) version.
<p>
<h2 class='tex'>2. Symmetrizing </h2> The direct-product as defined above has a permutation symmetry: \[ f^{\ox k}(\pi(x_1, \dots x_k)) = \pi(f^{\ox k}(x_1, \dots x_k)) \] for any permutation $\pi$.
<p>
The algorithm of Theorem <a href="#thmuniformDP">1</a> strongly takes advantage of this symmetry (indeed, the algorithm would not work as promised if we omitted the random permutations).<sup class='footnotemark'><a href='#footnote3' onclick="toggle_display_nojump('footnote3');">3</a></sup><span class='sidenote' id='footnote3'><a name='footnote3' href='#footnote3'>3.</a> Consider a $C'(x_1, \dots x_k)$ that is correct if $x_1$ lies in some $\eps$-density set, and random otherwise. Without the random permutations, $C_{A, v}(x)$ will always evaluate $C'(x, \dots)$, and produce no output for $(1-\epsilon)$-fraction of inputs $x$. </span> To simplify presentation, it helps to define the direct-product $f^k$ as a function over $k$-<b>multisets</b> of inputs, instead of over $k$-tuples of inputs. Following [<a href='#ref-IJKW10'>IJKW10</a>], for the remainder of this note, we will work in the setting of $k$-multisets, and denote the $k$-multiset direct product as $f^k$. That is, $f^k$ takes as input an (unordered) $k$-multiset $B = \{x_1, x_2, \dots, x_k\}$, and returns the $k$-tuple \[f^k(\{x_1, x_2, \dots, x_k\}) := (f(x_1), f(x_2), \dots, f(x_k))\]
<p>
We consider the probability measure induced by the uniform measure over tuples. That is, &ldquo;pick a random $k$-multiset of $U$&rdquo; means to generate a multiset by picking $k$ iid random elements from the universe $U$, and forming the (unordered) multiset containing them.<sup class='footnotemark'><a href='#footnote4' onclick="toggle_display_nojump('footnote4');">4</a></sup><span class='sidenote' id='footnote4'><a name='footnote4' href='#footnote4'>4.</a> So for example, for $k=3$ the multiset $\{a, a, a\}$ has lower probability of being drawn than $\{a, a, b\}$ for $a \neq b$. </span>
<p>
The notion of $\eps$-computing remains the same:<sup class='footnotemark'><a href='#footnote5' onclick="toggle_display_nojump('footnote5');">5</a></sup><span class='sidenote' id='footnote5'><a name='footnote5' href='#footnote5'>5.</a> For our purposes, having a randomized circuit that $\eps$-computes $f^{\ox k}$ is essentially equivalent to having a randomized circuit that $\eps$-computes $f^k$. The proofs will extend to randomized circuits, where we say $C$ $\eps$-computes $f$ if $\Pr_{C, x}[C(x) = f(x)] \geq \eps$, taken over randomness of $C$ as well as $x$. </span> A circuit $C'(B)$ $\epsilon$-computes $f^k$ if \[\Pr_{B \sim \text{random $k$-multiset}}[C'(B) = f^k(B)] \geq \epsilon\] Note that $C'$ is allowed to give different answers for the same element in a multiset, e.g. if $C'(\{a, a, a\}) = (y_1, y_2, y_3)$, the $y_i$s may all be distinct -- we don't take advantage of this symmetry.
<p>
<h2 class='tex'>3. Oracle Version </h2> Here we present and prove a simpler version of the algorithm, in the case when we also have access to an oracle for $f$. (This can be seen as a non-uniform version).
<blockquote><b>Theorem 2</b> <em> <a name="thmoracle"></a> Let $k \in \N, \eps &gt; 0$, and $f:\{0,1\}^n \to \{0, 1\}^\ell$. There is a PPT algorithm $\A^f$ with oracle access to $f$, with the following guarantees:

<ul> <li> <b>Input:</b> A circuit $C'$ that $\eps$-computes $f^k$. <li> <b>Output:</b> With probability $0.99$, output a circuit $C$ that $(1-\delta)$-computes $f$.
</ul>

 for $\delta = O(\log(k)/(\eps k))$. The circuit $C$ is of size $|C'|\poly(n, k, \log(1/\delta), 1/\eps)$. </em></blockquote>

<p>


<p>
The idea is, in Step 2 of Algorithm $\A$, we can generate the correct values $v$ for the inputs in set $A$, by querying the oracle. That is, we set $v := f(A)$ directly, instead of using our approximate circuit $C'$. In fact, if we have a perfect oracle for $f$ we can simplify the algorithm even further.
<p>
The algorithm is: <div class='framed'> $\mathcal{A}^f(C')$:
<p>


<ol> <li> Pick $T = O(\log(k)/\epsilon)$ random $(k-1)$-multisets $A_1, \dots A_T$, each $A_i$ containing $(k-1)$ random inputs from $\{0, 1\}^n$. <li> Query the $f$-oracle, and record the values of $v_{A_i} := \{f(x): x \in A_i\}$ for all sets $A_i$. <li> Output the circuit $C_{A, v}$ defined below (with the values $v_{A_i}$ on the subsets $A_i$ hardcoded).
</ol>

 </div>
<p>
$C_{A, v}$ is defined as the circuit:<br/>
 <div class='framed'> $C_{A, v}(x)$:
<p>
 For each $i = 1 \dots T = O(\log(k)/\epsilon)$:

<ol> <li> Let $B_i := \{x\} \cup A_i$. <li> Evaluate $C'(B_i)$. <li> If the answers of $C'(B_i)$ restricted to $A_i$ agree with the hardcoded values $v_{A_i} = f(A_i)$, then output $C'(B_i)|_x$, (the answer $C'$ gave for $x$), and stop.
</ol>

 Output an error if no output is produced after $T$ iterations. </div>
<p>
<em>Proof of Theorem <a href="#thmoracle">2</a>:</em>
<p>
<b>Parameters:</b> We will have $\delta = 10000\log(k)/(\epsilon k)$ and $T = 100 \log(k)/ \epsilon$. (Think of aiming for $\delta \approx 1/k$).
<p>
We will argue that \begin{equation} \label{eqn:main} \Pr_{\A, C, x}[C_{A, v}(x) \neq f(x)] \leq \delta / 100 \end{equation} Where the probability is over the randomness of algorithm $\A^f$ (random choice of sets $A_i$), and random input $x \in \{0, 1\}^n$. Then, by Markov \[\Pr_{\A}\left[ \Pr_{C, x}[C_{A, v}(x) \neq f(x)] &gt; \delta \right] \leq 1/100\] so the algorithm $\A^f$ will produce a good circuit $C_{A, v}$ except with probability $1/100$.
<p>
In the execution of circuit $C_{A, v}(x)$, let us say &ldquo;iteration $i$ fails&rdquo; if Step 3 of the circuit at iteration $i$ outputs a wrong answer. That is, iteration $i$ fails if $C'(B_i)$ is correct on the $(k-1)$ values in $A_i = B_i \setminus \{x\}$, but wrong on $x$.
<p>
Consider the probability that iteration 1 fails. Notice that the distribution of $(x, A_1, B_1)$ is equivalently generated as:
<p>
<table align = center><tr><td align=center> $\{(x, A_1, B_1)\}$ </td><td align=center> $\equiv$ </td><td align=center> $\{(x, A_1, B_1)\}$ </td></tr><tr><td align=center> $A_1 \sim$ random $(k-1)$-multiset </td><td align=center> </td><td align=center> $B_1 \sim$ random $k$-multiset </td></tr><tr><td align=center> $x \in \{0, 1\}^n$ </td><td align=center> </td><td align=center> $x \in B_1$</td></tr><tr><td align=center> $B_1 := \{x\} \cup A_1$ </td><td align=center> </td><td align=center> $A_1 := B_1 \setminus \{x\}$ </td></tr></table>
<p>
That is, we can think of first sampling a random $k$-multiset $B_1$, then sampling a random $x \in B_1$. Iteration 1 only returns an output when $C'(B_1)$ has at most $1$ wrong answer (since it checks correctness on the $(k-1)$ values of $A_1$). Thus iteration 1 only fails if the random $x \in B_1$ falls on this $1$ (of $k$) answers. So \begin{equation} \Pr_{x, A_1, B_1}[~\text{Iteration 1 fails}~] \leq \frac{1}{k} \end{equation}
<p>
 Now, we just union bound: \begin{align*} \Pr[\text{error}] &= \Pr_{\A, C, x}[C_{A, v}(x) \neq f(x)]\\ &\leq \Pr[\text{no output produced after $T$ iterations, or some iteration fails}]\\ &\leq \Pr[\text{no output produced}] + T \cdot \Pr[\text{Iteration 1 fails}]\\ &\leq \Pr[\text{no output produced}] + \frac{T}{k} \end{align*}
<p>
For our choice of $T, \delta$, the second term is $\frac{T}{k} \leq \delta / 200$. We will show the first term is $\leq \delta / 200$ as well, completing the proof.
<p>
<b>Produces output w.h.p.</b>
<p>
It remains to show that the circuit $C_{A, v}$ produces an output with high probability. In Step 3 of the circuit $C_{A, v}$, notice that if $C'$ is queried on a correct input $B_i$, it will pass the test and output a value.
<p>
The idea is: since $C'$ is correct on $\epsilon$-fraction of inputs, if we try $T = \Omega(\log(1/\delta)/\epsilon)$ iid random inputs, we will be sure to hit a correct input, except with probability $O(\delta)$. This doesn't quite work, since the inputs $B_i$ are not iid random (they all contain the input $x$) -- but this dependence is minimal, so it still works out.
<p>
Following [<a href='#ref-IJKW10'>IJKW10</a>], it helps to think in term of this bipartite graph. Define $G$ as a biregular bipartite graph between inputs $x \in \{0, 1\}^n$, and $k$-<b>tuples</b><sup class='footnotemark'><a href='#footnote6' onclick="toggle_display_nojump('footnote6');">6</a></sup><span class='sidenote' id='footnote6'><a name='footnote6' href='#footnote6'>6.</a> Going back to tuples just to simplify the notation, so we can deal with the uniform measure. </span> $B \in (\{0, 1\}^n)^k$, with an edge $(x, B)$ if $x \in B$. We can think of the circuit $C_{A, v}(x)$ as picking up to $T$ random neighbors of $x$ in the graph $G$, until hitting an input $B$ where $C'(B)$ is correct on all $B \setminus \{x\}$. We know that $\epsilon$-fraction of $k$-tuples $B$ are correct, and in fact we will show that almost all inputs $x$ have close to $\eps$-fraction of their neighbors as correct.
<p>
<p align=center><img width=350 src="/sources/uniform-DP/graph_color.png"></p>
<p>

<blockquote><b>Lemma 3</b> <em> <a name="lemnotbad"></a> There are at most $O(\delta)$-fraction of &ldquo;$\bad$&rdquo; inputs $x \in \{0, 1\}^n$ for which \[\Pr_{B \in N(x)}[C'(B) \text{ is correct}] \leq \epsilon/10\] </em></blockquote>

<p>

 This is sufficient to show that $\Pr[\text{no output produced}] \leq O(\delta)$, since for inputs $x$ that are not $\bad$, sampling $T = \Omega(\log(k)/\eps)$ iid neighbors of $x$ will hit a correct neighbor, except with probability $O(1/k) \leq O(\delta)$. <sup class='footnotemark'><a href='#footnote7' onclick="toggle_display_nojump('footnote7');">7</a></sup><span class='sidenote' id='footnote7'><a name='footnote7' href='#footnote7'>7.</a>  $(1-\eps/10)^{T} \leq e^{-T\eps / 10} \leq 1/k \leq \delta$.  </span>
<p>
It is easier to show the related property:
<blockquote><b>Lemma 4 (Mixing Lemma)</b> <em> <a name="lemmixing"></a> Let $H \subseteq \{0, 1\}^n$ be a set of inputs on the left of $G$, with the density of $H$ at least $\mu$. Then, except for some $2e^{-\Omega(\mu k)}$-fraction of tuples $B$, all tuples $B$ on the right of $G$ have \[\Pr_{x \in N(B)}[x \in H] = \mu \pm \mu/2\] </em></blockquote>

<p>

 <em>Proof of Lemma <a href="#lemmixing">4</a>:</em>  Drawing a uniformly random tuple $B$ on the right is exactly drawing $k$ iid samples of inputs $B := (x_1, x_2, \dots, x_k)$. Then, by definition of $G$, picking a random neighbor $x \in N(B)$ is just picking a random $x \in B$. Thus, it is sufficient to show that if we draw $k$ iid inputs $x_1, x_2, \dots, x_k$, the fraction of inputs that fall in $H$ is within a multiplicative factor $(1 \pm 1/2)$ of its expectation $\mu$ (with high probability). This follows immediately from Chernoff bounds. $$\tag*{$\blacksquare$}$$
<p>
From this, the above Lemma <a href="#lemnotbad">3</a> follows easily:
<p>
<em>Proof of Lemma <a href="#lemnotbad">3</a>:</em>  Let $\bad$ be the set of &ldquo;bad&rdquo; inputs $x$, where $\Pr_{B \in N(x)}[C'(B) \text{ is correct}] \leq \epsilon/10$. Suppose the density of $\bad$ is $\mu$. Let us count fraction of total edges in $G$ that go between $\bad$, and the set of correct tuples (which we call $\good$). By the mixing lemma, there are at least $(\epsilon - 2e^{\Omega(\mu k)})$ fraction of tuples $B^*$ with $\Pr_{x \in N(B^*)}[\text{$x$ is bad}] \geq \mu / 2$. So there are at least $(\epsilon - 2e^{\Omega(\mu k)}) (\mu/2)$ fraction of edges between the $\bad$ and $\good$ sets.
<p>
But, each bad input $x$ has at most $\eps/10$ fraction of edges into $\good$ by definition, so the fraction of $\bad \leftrightarrow \good$ edges is at most $\mu (\eps/10)$.
<p>
Thus we must have \begin{align*} (\epsilon - 2e^{-\Omega(\mu k)}) (\mu/2) &\leq \mu (\eps/10)\\ \implies \mu &\leq O(\log(1/\eps)/k) \end{align*} This gives $\mu \leq \delta/200$ for our choice of $\delta$. $$\tag*{$\blacksquare$}$$
<p>
This concludes the proof of correctness of the oracle version (Theorem <a href="#thmoracle">2</a>). $$\tag*{$\blacksquare$}$$
<p>
<h2 class='tex'>4. Closing Remarks </h2>
<p>


<ul> <li> Note that in the oracle version, we were able to output a good circuit with probability $0.99$, instead of w.p. $\Theta(\eps)$ as in the fully uniform version. This makes sense because if we have an $f$-oracle, we can &ldquo;check&rdquo; if our circuit is actually computing the desired $f$, so we don't run into the unique decoding problem. (Indeed, we can construct an optimal version of algorithm $\A^f$ of Theorem <a href="#thmoracle">2</a> from the algorithm $\A$ of Theorem <a href="#thmuniformDP">1</a> in a black-box way, by checking if the output circuit of $\A$ mostly agrees with $f$ on enough random inputs).
<p>
 <li> There were several simplifications we made from $\A$ to $\A^f$.<br/>
 (1) We queried the oracle for the hardcoded values $v$, instead of the circuit.<br/>
 (2) We hardcoded $(k-1)$-multisets instead of $(k/2)$-multisets.<br/>
 (3) We hardcoded $T$ iid multisets $\{A_i\}$, instead of just one multiset $A$.<br/>
 Note that we could not have done (2) without also doing (3) -- otherwise there would not have been enough mixing (the circuit would fail with probability close to $\eps$). Also, (3) would not have worked in the fully uniform case ($\A$, without the oracle) -- because then all the hardcoded sets will be correct with only very small probability.
<p>
 <li> The reason Theorem <a href="#thmoracle">2</a> has suboptimal parameters (eg, compare the setting of $\delta$ to Theorem <a href="#thmuniformDP">1</a>) is because our analysis used the loose union bound, instead of using the fact that circuit $C_{A, v}$, by only outputting values that pass a test, is doing rejection-sampling on a certain conditional probability space. The tight analysis in [<a href='#ref-IJKW10'>IJKW10</a>] takes advantage of this fact.
<p>
 <li> In the proof of Thereom <a href="#thmoracle">2</a>, we used a property of the graph $G$ that was essentially like an &ldquo;Expander Mixing Lemma&rdquo;. We may hope that if we replace $G$ with something sufficiently expander-like, we could get a derandomized direct-product theorem. Indeed, something like this is done in [<a href='#ref-IJKW10'>IJKW10</a>] (&ldquo;Uniform direct product theorems: simplified, optimized, and <i>derandomized</i>&rdquo;).
<p>
 <li> I think the oracle version is sufficient for the applications in [<a href='#ref-CIKK16'>CIKK16</a>], since there we have query access to the function $f$ we are trying to learn/compress.
<p>
 <li> For a good survey on direct-product for non-uniform hardness amplification, and the related &ldquo;Yao's XOR Lemma&rdquo;, see [<a href='#ref-GNW11'>GNW11</a>] (which includes at least 3 different proofs of the non-uniform XOR lemma). For a clean proof of Impagliazzo's Hardore Set theorem, which is used in some proofs of the XOR lemma, see for example Arora-Barak.
<p>

</ul>


<p>
<br><hr><h3>References</h3>
<p>
<a name='ref-CIKK16'>[CIKK16]</a>&emsp;Marco~L Carmosino, Russell Impagliazzo, Valentine Kabanets, and Antonina
  Kolokolova.
 Learning algorithms from natural proofs.
 In <em>LIPIcs-Leibniz International Proceedings in Informatics</em>,
  volume~50. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2016.
 URL:
  <a href="http://drops.dagstuhl.de/opus/volltexte/2016/5855/pdf/34.pdf">http://drops.dagstuhl.de/opus/volltexte/2016/5855/pdf/34.pdf</a>.
<p>

<p>
<a name='ref-GNW11'>[GNW11]</a>&emsp;Oded Goldreich, Noam Nisan, and Avi Wigderson.
 On yao's xor-lemma.
 In <em>Studies in Complexity and Cryptography. Miscellanea on the
  Interplay between Randomness and Computation</em>, pages 273--301. Springer,
  2011.
 URL: <a href="http://www.wisdom.weizmann.ac.il/~oded/COL/yao.pdf">http://www.wisdom.weizmann.ac.il/~oded/COL/yao.pdf</a>.
<p>

<p>
<a name='ref-IJKW10'>[IJKW10]</a>&emsp;Russell Impagliazzo, Ragesh Jaiswal, Valentine Kabanets, and Avi Wigderson.
 Uniform direct product theorems: simplified, optimized, and
  derandomized.
 <em>SIAM Journal on Computing</em>, 39(4):1637--1665, 2010.
 URL: <a href="http://www.cs.columbia.edu/~rjaiswal/IJKW-Full.pdf">http://www.cs.columbia.edu/~rjaiswal/IJKW-Full.pdf</a>.
<p>
